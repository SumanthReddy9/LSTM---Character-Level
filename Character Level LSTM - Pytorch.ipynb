{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype = np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape), n_labels)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    batch_size = n_seqs*n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    arr = arr[:batch_size*n_batches]\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps = 100, n_hidden = 256, n_layers = 2, drop_prob = 0.5, lr = 0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_steps = n_steps\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout = drop_prob, batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        \n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "        \n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        \n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        \n",
    "        self.fc.bias.data.fill_(0)\n",
    "        \n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs = 10, n_seqs = 10, n_steps = 50, lr = 0.001, clip=5, val_frac=0.1, cuda = False, print_every = 10):\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    print(val_idx)\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "        \n",
    "    counter = 0\n",
    "    \n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        h = net.init_hidden(n_seqs)\n",
    "        \n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            \n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "                \n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            \n",
    "            opt.step()\n",
    "            \n",
    "            \n",
    "            if counter%print_every == 0:\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                \n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    \n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    \n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    \n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
    "                    \n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = CharRNN(chars, n_hidden=512, n_layers=2)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1786699\n",
      "Epoch: 1/25... Step: 10... Loss: 3.3395... Val Loss: 3.3297\n",
      "Epoch: 1/25... Step: 20... Loss: 3.2291... Val Loss: 3.2229\n",
      "Epoch: 1/25... Step: 30... Loss: 3.1236... Val Loss: 3.0955\n",
      "Epoch: 1/25... Step: 40... Loss: 2.9309... Val Loss: 2.9310\n",
      "Epoch: 1/25... Step: 50... Loss: 2.7629... Val Loss: 2.7460\n",
      "Epoch: 1/25... Step: 60... Loss: 2.6320... Val Loss: 2.6336\n",
      "Epoch: 1/25... Step: 70... Loss: 2.5720... Val Loss: 2.5573\n",
      "Epoch: 1/25... Step: 80... Loss: 2.4992... Val Loss: 2.5009\n",
      "Epoch: 1/25... Step: 90... Loss: 2.4422... Val Loss: 2.4586\n",
      "Epoch: 1/25... Step: 100... Loss: 2.4024... Val Loss: 2.4241\n",
      "Epoch: 1/25... Step: 110... Loss: 2.3639... Val Loss: 2.3916\n",
      "Epoch: 1/25... Step: 120... Loss: 2.3545... Val Loss: 2.3615\n",
      "Epoch: 1/25... Step: 130... Loss: 2.3358... Val Loss: 2.3318\n",
      "Epoch: 1/25... Step: 140... Loss: 2.2406... Val Loss: 2.3067\n",
      "Epoch: 1/25... Step: 150... Loss: 2.2694... Val Loss: 2.2823\n",
      "Epoch: 1/25... Step: 160... Loss: 2.2060... Val Loss: 2.2600\n",
      "Epoch: 1/25... Step: 170... Loss: 2.2295... Val Loss: 2.2363\n",
      "Epoch: 2/25... Step: 180... Loss: 2.1683... Val Loss: 2.2126\n",
      "Epoch: 2/25... Step: 190... Loss: 2.1313... Val Loss: 2.1955\n",
      "Epoch: 2/25... Step: 200... Loss: 2.1400... Val Loss: 2.1710\n",
      "Epoch: 2/25... Step: 210... Loss: 2.1363... Val Loss: 2.1509\n",
      "Epoch: 2/25... Step: 220... Loss: 2.0920... Val Loss: 2.1375\n",
      "Epoch: 2/25... Step: 230... Loss: 2.0977... Val Loss: 2.1808\n",
      "Epoch: 2/25... Step: 240... Loss: 2.0393... Val Loss: 2.0982\n",
      "Epoch: 2/25... Step: 250... Loss: 2.0283... Val Loss: 2.0796\n",
      "Epoch: 2/25... Step: 260... Loss: 2.0344... Val Loss: 2.0653\n",
      "Epoch: 2/25... Step: 270... Loss: 1.9820... Val Loss: 2.0487\n",
      "Epoch: 2/25... Step: 280... Loss: 2.0061... Val Loss: 2.0355\n",
      "Epoch: 2/25... Step: 290... Loss: 1.9867... Val Loss: 2.0183\n",
      "Epoch: 2/25... Step: 300... Loss: 1.9345... Val Loss: 2.0126\n",
      "Epoch: 2/25... Step: 310... Loss: 1.9606... Val Loss: 1.9923\n",
      "Epoch: 2/25... Step: 320... Loss: 1.9107... Val Loss: 1.9806\n",
      "Epoch: 2/25... Step: 330... Loss: 1.8900... Val Loss: 1.9673\n",
      "Epoch: 2/25... Step: 340... Loss: 1.9278... Val Loss: 1.9582\n",
      "Epoch: 2/25... Step: 350... Loss: 1.9042... Val Loss: 1.9516\n",
      "Epoch: 3/25... Step: 360... Loss: 1.8734... Val Loss: 1.9376\n",
      "Epoch: 3/25... Step: 370... Loss: 1.8857... Val Loss: 1.9232\n",
      "Epoch: 3/25... Step: 380... Loss: 1.8639... Val Loss: 1.9180\n",
      "Epoch: 3/25... Step: 390... Loss: 1.8760... Val Loss: 1.9012\n",
      "Epoch: 3/25... Step: 400... Loss: 1.8491... Val Loss: 1.8952\n",
      "Epoch: 3/25... Step: 410... Loss: 1.8042... Val Loss: 1.8855\n",
      "Epoch: 3/25... Step: 420... Loss: 1.7952... Val Loss: 1.8812\n",
      "Epoch: 3/25... Step: 430... Loss: 1.8561... Val Loss: 1.8665\n",
      "Epoch: 3/25... Step: 440... Loss: 1.8011... Val Loss: 1.8616\n",
      "Epoch: 3/25... Step: 450... Loss: 1.8004... Val Loss: 1.8511\n",
      "Epoch: 3/25... Step: 460... Loss: 1.7481... Val Loss: 1.8410\n",
      "Epoch: 3/25... Step: 470... Loss: 1.7949... Val Loss: 1.8331\n",
      "Epoch: 3/25... Step: 480... Loss: 1.7670... Val Loss: 1.8433\n",
      "Epoch: 3/25... Step: 490... Loss: 1.7485... Val Loss: 1.8213\n",
      "Epoch: 3/25... Step: 500... Loss: 1.7725... Val Loss: 1.8133\n",
      "Epoch: 3/25... Step: 510... Loss: 1.7411... Val Loss: 1.8049\n",
      "Epoch: 3/25... Step: 520... Loss: 1.7438... Val Loss: 1.7967\n",
      "Epoch: 3/25... Step: 530... Loss: 1.7714... Val Loss: 1.7906\n",
      "Epoch: 4/25... Step: 540... Loss: 1.7466... Val Loss: 1.7786\n",
      "Epoch: 4/25... Step: 550... Loss: 1.6636... Val Loss: 1.7770\n",
      "Epoch: 4/25... Step: 560... Loss: 1.7017... Val Loss: 1.7681\n",
      "Epoch: 4/25... Step: 570... Loss: 1.6951... Val Loss: 1.7590\n",
      "Epoch: 4/25... Step: 580... Loss: 1.6883... Val Loss: 1.7555\n",
      "Epoch: 4/25... Step: 590... Loss: 1.6731... Val Loss: 1.7542\n",
      "Epoch: 4/25... Step: 600... Loss: 1.6847... Val Loss: 1.7446\n",
      "Epoch: 4/25... Step: 610... Loss: 1.6992... Val Loss: 1.7409\n",
      "Epoch: 4/25... Step: 620... Loss: 1.6555... Val Loss: 1.7338\n",
      "Epoch: 4/25... Step: 630... Loss: 1.6165... Val Loss: 1.7273\n",
      "Epoch: 4/25... Step: 640... Loss: 1.6902... Val Loss: 1.7165\n",
      "Epoch: 4/25... Step: 650... Loss: 1.6057... Val Loss: 1.7121\n",
      "Epoch: 4/25... Step: 660... Loss: 1.6456... Val Loss: 1.7114\n",
      "Epoch: 4/25... Step: 670... Loss: 1.6370... Val Loss: 1.7111\n",
      "Epoch: 4/25... Step: 680... Loss: 1.6300... Val Loss: 1.7038\n",
      "Epoch: 4/25... Step: 690... Loss: 1.6311... Val Loss: 1.6940\n",
      "Epoch: 4/25... Step: 700... Loss: 1.6617... Val Loss: 1.6889\n",
      "Epoch: 4/25... Step: 710... Loss: 1.5512... Val Loss: 1.6822\n",
      "Epoch: 5/25... Step: 720... Loss: 1.5853... Val Loss: 1.6810\n",
      "Epoch: 5/25... Step: 730... Loss: 1.5697... Val Loss: 1.6831\n",
      "Epoch: 5/25... Step: 740... Loss: 1.6180... Val Loss: 1.6751\n",
      "Epoch: 5/25... Step: 750... Loss: 1.5811... Val Loss: 1.6693\n",
      "Epoch: 5/25... Step: 760... Loss: 1.6201... Val Loss: 1.6651\n",
      "Epoch: 5/25... Step: 770... Loss: 1.5603... Val Loss: 1.6594\n",
      "Epoch: 5/25... Step: 780... Loss: 1.5527... Val Loss: 1.6577\n",
      "Epoch: 5/25... Step: 790... Loss: 1.5896... Val Loss: 1.6522\n",
      "Epoch: 5/25... Step: 800... Loss: 1.5547... Val Loss: 1.6473\n",
      "Epoch: 5/25... Step: 810... Loss: 1.5258... Val Loss: 1.6488\n",
      "Epoch: 5/25... Step: 820... Loss: 1.5378... Val Loss: 1.6402\n",
      "Epoch: 5/25... Step: 830... Loss: 1.5230... Val Loss: 1.6336\n",
      "Epoch: 5/25... Step: 840... Loss: 1.5467... Val Loss: 1.6302\n",
      "Epoch: 5/25... Step: 850... Loss: 1.5328... Val Loss: 1.6286\n",
      "Epoch: 5/25... Step: 860... Loss: 1.5580... Val Loss: 1.6265\n",
      "Epoch: 5/25... Step: 870... Loss: 1.5538... Val Loss: 1.6212\n",
      "Epoch: 5/25... Step: 880... Loss: 1.5441... Val Loss: 1.6149\n",
      "Epoch: 5/25... Step: 890... Loss: 1.5540... Val Loss: 1.6119\n",
      "Epoch: 6/25... Step: 900... Loss: 1.5150... Val Loss: 1.6141\n",
      "Epoch: 6/25... Step: 910... Loss: 1.5093... Val Loss: 1.6128\n",
      "Epoch: 6/25... Step: 920... Loss: 1.4954... Val Loss: 1.6056\n",
      "Epoch: 6/25... Step: 930... Loss: 1.5123... Val Loss: 1.5994\n",
      "Epoch: 6/25... Step: 940... Loss: 1.5064... Val Loss: 1.5979\n",
      "Epoch: 6/25... Step: 950... Loss: 1.5147... Val Loss: 1.5953\n",
      "Epoch: 6/25... Step: 960... Loss: 1.5316... Val Loss: 1.5900\n",
      "Epoch: 6/25... Step: 970... Loss: 1.5119... Val Loss: 1.5899\n",
      "Epoch: 6/25... Step: 980... Loss: 1.4878... Val Loss: 1.5826\n",
      "Epoch: 6/25... Step: 990... Loss: 1.4254... Val Loss: 1.5840\n",
      "Epoch: 6/25... Step: 1000... Loss: 1.4895... Val Loss: 1.5792\n",
      "Epoch: 6/25... Step: 1010... Loss: 1.4948... Val Loss: 1.5851\n",
      "Epoch: 6/25... Step: 1020... Loss: 1.5041... Val Loss: 1.5744\n",
      "Epoch: 6/25... Step: 1030... Loss: 1.4398... Val Loss: 1.5695\n",
      "Epoch: 6/25... Step: 1040... Loss: 1.4998... Val Loss: 1.5690\n",
      "Epoch: 6/25... Step: 1050... Loss: 1.4703... Val Loss: 1.5632\n",
      "Epoch: 6/25... Step: 1060... Loss: 1.4748... Val Loss: 1.5576\n",
      "Epoch: 7/25... Step: 1070... Loss: 1.4577... Val Loss: 1.5580\n",
      "Epoch: 7/25... Step: 1080... Loss: 1.4405... Val Loss: 1.5561\n",
      "Epoch: 7/25... Step: 1090... Loss: 1.4453... Val Loss: 1.5624\n",
      "Epoch: 7/25... Step: 1100... Loss: 1.4799... Val Loss: 1.5659\n",
      "Epoch: 7/25... Step: 1110... Loss: 1.4488... Val Loss: 1.5649\n",
      "Epoch: 7/25... Step: 1120... Loss: 1.4676... Val Loss: 1.5497\n",
      "Epoch: 7/25... Step: 1130... Loss: 1.4559... Val Loss: 1.5491\n",
      "Epoch: 7/25... Step: 1140... Loss: 1.4158... Val Loss: 1.5416\n",
      "Epoch: 7/25... Step: 1150... Loss: 1.4520... Val Loss: 1.5437\n",
      "Epoch: 7/25... Step: 1160... Loss: 1.4441... Val Loss: 1.5374\n",
      "Epoch: 7/25... Step: 1170... Loss: 1.4481... Val Loss: 1.5356\n",
      "Epoch: 7/25... Step: 1180... Loss: 1.4523... Val Loss: 1.5340\n",
      "Epoch: 7/25... Step: 1190... Loss: 1.4111... Val Loss: 1.5341\n",
      "Epoch: 7/25... Step: 1200... Loss: 1.4512... Val Loss: 1.5378\n",
      "Epoch: 7/25... Step: 1210... Loss: 1.3987... Val Loss: 1.5277\n",
      "Epoch: 7/25... Step: 1220... Loss: 1.4176... Val Loss: 1.5438\n",
      "Epoch: 7/25... Step: 1230... Loss: 1.4697... Val Loss: 1.5233\n",
      "Epoch: 7/25... Step: 1240... Loss: 1.4338... Val Loss: 1.5244\n",
      "Epoch: 8/25... Step: 1250... Loss: 1.4005... Val Loss: 1.5221\n",
      "Epoch: 8/25... Step: 1260... Loss: 1.4145... Val Loss: 1.5149\n",
      "Epoch: 8/25... Step: 1270... Loss: 1.4130... Val Loss: 1.5152\n",
      "Epoch: 8/25... Step: 1280... Loss: 1.4200... Val Loss: 1.5127\n",
      "Epoch: 8/25... Step: 1290... Loss: 1.4435... Val Loss: 1.5124\n",
      "Epoch: 8/25... Step: 1300... Loss: 1.3763... Val Loss: 1.5129\n",
      "Epoch: 8/25... Step: 1310... Loss: 1.3710... Val Loss: 1.5067\n",
      "Epoch: 8/25... Step: 1320... Loss: 1.4417... Val Loss: 1.5064\n",
      "Epoch: 8/25... Step: 1330... Loss: 1.4092... Val Loss: 1.5015\n",
      "Epoch: 8/25... Step: 1340... Loss: 1.3988... Val Loss: 1.5017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/25... Step: 1350... Loss: 1.3613... Val Loss: 1.5041\n",
      "Epoch: 8/25... Step: 1360... Loss: 1.3957... Val Loss: 1.4984\n",
      "Epoch: 8/25... Step: 1370... Loss: 1.3858... Val Loss: 1.4930\n",
      "Epoch: 8/25... Step: 1380... Loss: 1.3741... Val Loss: 1.4989\n",
      "Epoch: 8/25... Step: 1390... Loss: 1.3961... Val Loss: 1.5036\n",
      "Epoch: 8/25... Step: 1400... Loss: 1.3761... Val Loss: 1.4988\n",
      "Epoch: 8/25... Step: 1410... Loss: 1.3804... Val Loss: 1.4965\n",
      "Epoch: 8/25... Step: 1420... Loss: 1.3876... Val Loss: 1.4954\n",
      "Epoch: 9/25... Step: 1430... Loss: 1.4046... Val Loss: 1.4889\n",
      "Epoch: 9/25... Step: 1440... Loss: 1.3457... Val Loss: 1.4873\n",
      "Epoch: 9/25... Step: 1450... Loss: 1.3604... Val Loss: 1.4860\n",
      "Epoch: 9/25... Step: 1460... Loss: 1.3820... Val Loss: 1.4802\n",
      "Epoch: 9/25... Step: 1470... Loss: 1.3668... Val Loss: 1.4837\n",
      "Epoch: 9/25... Step: 1480... Loss: 1.3557... Val Loss: 1.4812\n",
      "Epoch: 9/25... Step: 1490... Loss: 1.3854... Val Loss: 1.4831\n",
      "Epoch: 9/25... Step: 1500... Loss: 1.3990... Val Loss: 1.4723\n",
      "Epoch: 9/25... Step: 1510... Loss: 1.3630... Val Loss: 1.4741\n",
      "Epoch: 9/25... Step: 1520... Loss: 1.3336... Val Loss: 1.4816\n",
      "Epoch: 9/25... Step: 1530... Loss: 1.3924... Val Loss: 1.4718\n",
      "Epoch: 9/25... Step: 1540... Loss: 1.3199... Val Loss: 1.4709\n",
      "Epoch: 9/25... Step: 1550... Loss: 1.3632... Val Loss: 1.4725\n",
      "Epoch: 9/25... Step: 1560... Loss: 1.3490... Val Loss: 1.4770\n",
      "Epoch: 9/25... Step: 1570... Loss: 1.3550... Val Loss: 1.4668\n",
      "Epoch: 9/25... Step: 1580... Loss: 1.3556... Val Loss: 1.4627\n",
      "Epoch: 9/25... Step: 1590... Loss: 1.3754... Val Loss: 1.4657\n",
      "Epoch: 9/25... Step: 1600... Loss: 1.2927... Val Loss: 1.4617\n",
      "Epoch: 10/25... Step: 1610... Loss: 1.3190... Val Loss: 1.4623\n",
      "Epoch: 10/25... Step: 1620... Loss: 1.2944... Val Loss: 1.4632\n",
      "Epoch: 10/25... Step: 1630... Loss: 1.3650... Val Loss: 1.4587\n",
      "Epoch: 10/25... Step: 1640... Loss: 1.3261... Val Loss: 1.4641\n",
      "Epoch: 10/25... Step: 1650... Loss: 1.3665... Val Loss: 1.4575\n",
      "Epoch: 10/25... Step: 1660... Loss: 1.3393... Val Loss: 1.4608\n",
      "Epoch: 10/25... Step: 1670... Loss: 1.3376... Val Loss: 1.4588\n",
      "Epoch: 10/25... Step: 1680... Loss: 1.3493... Val Loss: 1.4513\n",
      "Epoch: 10/25... Step: 1690... Loss: 1.3294... Val Loss: 1.4523\n",
      "Epoch: 10/25... Step: 1700... Loss: 1.3111... Val Loss: 1.4515\n",
      "Epoch: 10/25... Step: 1710... Loss: 1.3376... Val Loss: 1.4435\n",
      "Epoch: 10/25... Step: 1720... Loss: 1.3032... Val Loss: 1.4535\n",
      "Epoch: 10/25... Step: 1730... Loss: 1.3210... Val Loss: 1.4484\n",
      "Epoch: 10/25... Step: 1740... Loss: 1.3187... Val Loss: 1.4497\n",
      "Epoch: 10/25... Step: 1750... Loss: 1.3401... Val Loss: 1.4493\n",
      "Epoch: 10/25... Step: 1760... Loss: 1.3408... Val Loss: 1.4438\n",
      "Epoch: 10/25... Step: 1770... Loss: 1.3384... Val Loss: 1.4472\n",
      "Epoch: 10/25... Step: 1780... Loss: 1.3576... Val Loss: 1.4432\n",
      "Epoch: 11/25... Step: 1790... Loss: 1.3114... Val Loss: 1.4405\n",
      "Epoch: 11/25... Step: 1800... Loss: 1.2944... Val Loss: 1.4460\n",
      "Epoch: 11/25... Step: 1810... Loss: 1.2962... Val Loss: 1.4414\n",
      "Epoch: 11/25... Step: 1820... Loss: 1.3177... Val Loss: 1.4495\n",
      "Epoch: 11/25... Step: 1830... Loss: 1.3071... Val Loss: 1.4400\n",
      "Epoch: 11/25... Step: 1840... Loss: 1.3126... Val Loss: 1.4353\n",
      "Epoch: 11/25... Step: 1850... Loss: 1.3251... Val Loss: 1.4379\n",
      "Epoch: 11/25... Step: 1860... Loss: 1.3154... Val Loss: 1.4356\n",
      "Epoch: 11/25... Step: 1870... Loss: 1.3004... Val Loss: 1.4332\n",
      "Epoch: 11/25... Step: 1880... Loss: 1.2429... Val Loss: 1.4348\n",
      "Epoch: 11/25... Step: 1890... Loss: 1.3012... Val Loss: 1.4295\n",
      "Epoch: 11/25... Step: 1900... Loss: 1.2968... Val Loss: 1.4326\n",
      "Epoch: 11/25... Step: 1910... Loss: 1.3190... Val Loss: 1.4323\n",
      "Epoch: 11/25... Step: 1920... Loss: 1.2618... Val Loss: 1.4332\n",
      "Epoch: 11/25... Step: 1930... Loss: 1.3090... Val Loss: 1.4371\n",
      "Epoch: 11/25... Step: 1940... Loss: 1.3200... Val Loss: 1.4336\n",
      "Epoch: 11/25... Step: 1950... Loss: 1.3194... Val Loss: 1.4363\n",
      "Epoch: 12/25... Step: 1960... Loss: 1.2764... Val Loss: 1.4270\n",
      "Epoch: 12/25... Step: 1970... Loss: 1.2819... Val Loss: 1.4244\n",
      "Epoch: 12/25... Step: 1980... Loss: 1.2792... Val Loss: 1.4324\n",
      "Epoch: 12/25... Step: 1990... Loss: 1.2996... Val Loss: 1.4246\n",
      "Epoch: 12/25... Step: 2000... Loss: 1.2759... Val Loss: 1.4265\n",
      "Epoch: 12/25... Step: 2010... Loss: 1.3067... Val Loss: 1.4258\n",
      "Epoch: 12/25... Step: 2020... Loss: 1.2761... Val Loss: 1.4213\n",
      "Epoch: 12/25... Step: 2030... Loss: 1.2537... Val Loss: 1.4261\n",
      "Epoch: 12/25... Step: 2040... Loss: 1.3042... Val Loss: 1.4222\n",
      "Epoch: 12/25... Step: 2050... Loss: 1.2966... Val Loss: 1.4206\n",
      "Epoch: 12/25... Step: 2060... Loss: 1.3015... Val Loss: 1.4204\n",
      "Epoch: 12/25... Step: 2070... Loss: 1.2939... Val Loss: 1.4171\n",
      "Epoch: 12/25... Step: 2080... Loss: 1.2602... Val Loss: 1.4188\n",
      "Epoch: 12/25... Step: 2090... Loss: 1.2902... Val Loss: 1.4164\n",
      "Epoch: 12/25... Step: 2100... Loss: 1.2547... Val Loss: 1.4094\n",
      "Epoch: 12/25... Step: 2110... Loss: 1.2583... Val Loss: 1.4169\n",
      "Epoch: 12/25... Step: 2120... Loss: 1.3267... Val Loss: 1.4096\n",
      "Epoch: 12/25... Step: 2130... Loss: 1.2770... Val Loss: 1.4139\n",
      "Epoch: 13/25... Step: 2140... Loss: 1.2578... Val Loss: 1.4127\n",
      "Epoch: 13/25... Step: 2150... Loss: 1.2690... Val Loss: 1.4105\n",
      "Epoch: 13/25... Step: 2160... Loss: 1.2790... Val Loss: 1.4181\n",
      "Epoch: 13/25... Step: 2170... Loss: 1.2750... Val Loss: 1.4086\n",
      "Epoch: 13/25... Step: 2180... Loss: 1.3062... Val Loss: 1.4234\n",
      "Epoch: 13/25... Step: 2190... Loss: 1.2416... Val Loss: 1.4124\n",
      "Epoch: 13/25... Step: 2200... Loss: 1.2253... Val Loss: 1.4096\n",
      "Epoch: 13/25... Step: 2210... Loss: 1.3124... Val Loss: 1.4112\n",
      "Epoch: 13/25... Step: 2220... Loss: 1.2739... Val Loss: 1.4055\n",
      "Epoch: 13/25... Step: 2230... Loss: 1.2727... Val Loss: 1.4112\n",
      "Epoch: 13/25... Step: 2240... Loss: 1.2387... Val Loss: 1.4144\n",
      "Epoch: 13/25... Step: 2250... Loss: 1.2619... Val Loss: 1.4015\n",
      "Epoch: 13/25... Step: 2260... Loss: 1.2546... Val Loss: 1.4027\n",
      "Epoch: 13/25... Step: 2270... Loss: 1.2414... Val Loss: 1.4015\n",
      "Epoch: 13/25... Step: 2280... Loss: 1.2593... Val Loss: 1.4097\n",
      "Epoch: 13/25... Step: 2290... Loss: 1.2655... Val Loss: 1.4082\n",
      "Epoch: 13/25... Step: 2300... Loss: 1.2609... Val Loss: 1.4043\n",
      "Epoch: 13/25... Step: 2310... Loss: 1.2626... Val Loss: 1.3943\n",
      "Epoch: 14/25... Step: 2320... Loss: 1.2848... Val Loss: 1.3996\n",
      "Epoch: 14/25... Step: 2330... Loss: 1.2234... Val Loss: 1.3980\n",
      "Epoch: 14/25... Step: 2340... Loss: 1.2514... Val Loss: 1.3964\n",
      "Epoch: 14/25... Step: 2350... Loss: 1.2715... Val Loss: 1.3954\n",
      "Epoch: 14/25... Step: 2360... Loss: 1.2435... Val Loss: 1.3928\n",
      "Epoch: 14/25... Step: 2370... Loss: 1.2403... Val Loss: 1.3954\n",
      "Epoch: 14/25... Step: 2380... Loss: 1.2551... Val Loss: 1.3956\n",
      "Epoch: 14/25... Step: 2390... Loss: 1.2817... Val Loss: 1.3922\n",
      "Epoch: 14/25... Step: 2400... Loss: 1.2426... Val Loss: 1.3874\n",
      "Epoch: 14/25... Step: 2410... Loss: 1.2099... Val Loss: 1.3985\n",
      "Epoch: 14/25... Step: 2420... Loss: 1.2781... Val Loss: 1.3987\n",
      "Epoch: 14/25... Step: 2430... Loss: 1.2129... Val Loss: 1.3936\n",
      "Epoch: 14/25... Step: 2440... Loss: 1.2473... Val Loss: 1.3926\n",
      "Epoch: 14/25... Step: 2450... Loss: 1.2287... Val Loss: 1.3872\n",
      "Epoch: 14/25... Step: 2460... Loss: 1.2436... Val Loss: 1.3906\n",
      "Epoch: 14/25... Step: 2470... Loss: 1.2385... Val Loss: 1.3881\n",
      "Epoch: 14/25... Step: 2480... Loss: 1.2770... Val Loss: 1.3858\n",
      "Epoch: 14/25... Step: 2490... Loss: 1.1917... Val Loss: 1.3839\n",
      "Epoch: 15/25... Step: 2500... Loss: 1.2229... Val Loss: 1.3882\n",
      "Epoch: 15/25... Step: 2510... Loss: 1.1909... Val Loss: 1.3901\n",
      "Epoch: 15/25... Step: 2520... Loss: 1.2553... Val Loss: 1.3828\n",
      "Epoch: 15/25... Step: 2530... Loss: 1.2221... Val Loss: 1.3848\n",
      "Epoch: 15/25... Step: 2540... Loss: 1.2579... Val Loss: 1.3801\n",
      "Epoch: 15/25... Step: 2550... Loss: 1.2157... Val Loss: 1.3841\n",
      "Epoch: 15/25... Step: 2560... Loss: 1.2276... Val Loss: 1.3878\n",
      "Epoch: 15/25... Step: 2570... Loss: 1.2388... Val Loss: 1.3829\n",
      "Epoch: 15/25... Step: 2580... Loss: 1.2174... Val Loss: 1.3781\n",
      "Epoch: 15/25... Step: 2590... Loss: 1.2091... Val Loss: 1.3839\n",
      "Epoch: 15/25... Step: 2600... Loss: 1.2332... Val Loss: 1.3788\n",
      "Epoch: 15/25... Step: 2610... Loss: 1.2052... Val Loss: 1.3751\n",
      "Epoch: 15/25... Step: 2620... Loss: 1.2200... Val Loss: 1.3751\n",
      "Epoch: 15/25... Step: 2630... Loss: 1.2106... Val Loss: 1.3868\n",
      "Epoch: 15/25... Step: 2640... Loss: 1.2360... Val Loss: 1.3785\n",
      "Epoch: 15/25... Step: 2650... Loss: 1.2315... Val Loss: 1.3779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/25... Step: 2660... Loss: 1.2461... Val Loss: 1.3745\n",
      "Epoch: 15/25... Step: 2670... Loss: 1.2626... Val Loss: 1.3778\n",
      "Epoch: 16/25... Step: 2680... Loss: 1.2097... Val Loss: 1.3718\n",
      "Epoch: 16/25... Step: 2690... Loss: 1.1998... Val Loss: 1.3763\n",
      "Epoch: 16/25... Step: 2700... Loss: 1.2040... Val Loss: 1.3698\n",
      "Epoch: 16/25... Step: 2710... Loss: 1.2180... Val Loss: 1.3725\n",
      "Epoch: 16/25... Step: 2720... Loss: 1.2182... Val Loss: 1.3711\n",
      "Epoch: 16/25... Step: 2730... Loss: 1.2211... Val Loss: 1.3716\n",
      "Epoch: 16/25... Step: 2740... Loss: 1.2236... Val Loss: 1.3747\n",
      "Epoch: 16/25... Step: 2750... Loss: 1.2327... Val Loss: 1.3766\n",
      "Epoch: 16/25... Step: 2760... Loss: 1.2135... Val Loss: 1.3687\n",
      "Epoch: 16/25... Step: 2770... Loss: 1.1591... Val Loss: 1.3756\n",
      "Epoch: 16/25... Step: 2780... Loss: 1.2153... Val Loss: 1.3684\n",
      "Epoch: 16/25... Step: 2790... Loss: 1.2006... Val Loss: 1.3708\n",
      "Epoch: 16/25... Step: 2800... Loss: 1.2159... Val Loss: 1.3664\n",
      "Epoch: 16/25... Step: 2810... Loss: 1.1844... Val Loss: 1.3800\n",
      "Epoch: 16/25... Step: 2820... Loss: 1.2176... Val Loss: 1.3706\n",
      "Epoch: 16/25... Step: 2830... Loss: 1.2143... Val Loss: 1.3645\n",
      "Epoch: 16/25... Step: 2840... Loss: 1.2126... Val Loss: 1.3685\n",
      "Epoch: 17/25... Step: 2850... Loss: 1.2002... Val Loss: 1.3739\n",
      "Epoch: 17/25... Step: 2860... Loss: 1.1698... Val Loss: 1.3682\n",
      "Epoch: 17/25... Step: 2870... Loss: 1.1801... Val Loss: 1.3721\n",
      "Epoch: 17/25... Step: 2880... Loss: 1.2084... Val Loss: 1.3639\n",
      "Epoch: 17/25... Step: 2890... Loss: 1.1865... Val Loss: 1.3620\n",
      "Epoch: 17/25... Step: 2900... Loss: 1.2189... Val Loss: 1.3619\n",
      "Epoch: 17/25... Step: 2910... Loss: 1.1959... Val Loss: 1.3633\n",
      "Epoch: 17/25... Step: 2920... Loss: 1.1721... Val Loss: 1.3665\n",
      "Epoch: 17/25... Step: 2930... Loss: 1.2110... Val Loss: 1.3641\n",
      "Epoch: 17/25... Step: 2940... Loss: 1.2026... Val Loss: 1.3651\n",
      "Epoch: 17/25... Step: 2950... Loss: 1.2207... Val Loss: 1.3637\n",
      "Epoch: 17/25... Step: 2960... Loss: 1.2122... Val Loss: 1.3624\n",
      "Epoch: 17/25... Step: 2970... Loss: 1.1766... Val Loss: 1.3615\n",
      "Epoch: 17/25... Step: 2980... Loss: 1.2096... Val Loss: 1.3619\n",
      "Epoch: 17/25... Step: 2990... Loss: 1.1629... Val Loss: 1.3613\n",
      "Epoch: 17/25... Step: 3000... Loss: 1.1790... Val Loss: 1.3621\n",
      "Epoch: 17/25... Step: 3010... Loss: 1.2226... Val Loss: 1.3617\n",
      "Epoch: 17/25... Step: 3020... Loss: 1.1968... Val Loss: 1.3571\n",
      "Epoch: 18/25... Step: 3030... Loss: 1.1761... Val Loss: 1.3574\n",
      "Epoch: 18/25... Step: 3040... Loss: 1.1805... Val Loss: 1.3598\n",
      "Epoch: 18/25... Step: 3050... Loss: 1.1894... Val Loss: 1.3658\n",
      "Epoch: 18/25... Step: 3060... Loss: 1.1969... Val Loss: 1.3618\n",
      "Epoch: 18/25... Step: 3070... Loss: 1.2202... Val Loss: 1.3576\n",
      "Epoch: 18/25... Step: 3080... Loss: 1.1575... Val Loss: 1.3633\n",
      "Epoch: 18/25... Step: 3090... Loss: 1.1439... Val Loss: 1.3641\n",
      "Epoch: 18/25... Step: 3100... Loss: 1.2341... Val Loss: 1.3656\n",
      "Epoch: 18/25... Step: 3110... Loss: 1.1981... Val Loss: 1.3686\n",
      "Epoch: 18/25... Step: 3120... Loss: 1.1994... Val Loss: 1.3587\n",
      "Epoch: 18/25... Step: 3130... Loss: 1.1671... Val Loss: 1.3571\n",
      "Epoch: 18/25... Step: 3140... Loss: 1.1802... Val Loss: 1.3629\n",
      "Epoch: 18/25... Step: 3150... Loss: 1.1764... Val Loss: 1.3569\n",
      "Epoch: 18/25... Step: 3160... Loss: 1.1675... Val Loss: 1.3522\n",
      "Epoch: 18/25... Step: 3170... Loss: 1.1798... Val Loss: 1.3540\n",
      "Epoch: 18/25... Step: 3180... Loss: 1.1835... Val Loss: 1.3560\n",
      "Epoch: 18/25... Step: 3190... Loss: 1.1771... Val Loss: 1.3507\n",
      "Epoch: 18/25... Step: 3200... Loss: 1.1839... Val Loss: 1.3529\n",
      "Epoch: 19/25... Step: 3210... Loss: 1.2042... Val Loss: 1.3518\n",
      "Epoch: 19/25... Step: 3220... Loss: 1.1529... Val Loss: 1.3561\n",
      "Epoch: 19/25... Step: 3230... Loss: 1.1590... Val Loss: 1.3560\n",
      "Epoch: 19/25... Step: 3240... Loss: 1.1885... Val Loss: 1.3519\n",
      "Epoch: 19/25... Step: 3250... Loss: 1.1712... Val Loss: 1.3553\n",
      "Epoch: 19/25... Step: 3260... Loss: 1.1672... Val Loss: 1.3591\n",
      "Epoch: 19/25... Step: 3270... Loss: 1.1896... Val Loss: 1.3582\n",
      "Epoch: 19/25... Step: 3280... Loss: 1.2008... Val Loss: 1.3529\n",
      "Epoch: 19/25... Step: 3290... Loss: 1.1761... Val Loss: 1.3534\n",
      "Epoch: 19/25... Step: 3300... Loss: 1.1487... Val Loss: 1.3537\n",
      "Epoch: 19/25... Step: 3310... Loss: 1.2069... Val Loss: 1.3516\n",
      "Epoch: 19/25... Step: 3320... Loss: 1.1230... Val Loss: 1.3566\n",
      "Epoch: 19/25... Step: 3330... Loss: 1.1704... Val Loss: 1.3518\n",
      "Epoch: 19/25... Step: 3340... Loss: 1.1713... Val Loss: 1.3500\n",
      "Epoch: 19/25... Step: 3350... Loss: 1.1764... Val Loss: 1.3489\n",
      "Epoch: 19/25... Step: 3360... Loss: 1.1767... Val Loss: 1.3450\n",
      "Epoch: 19/25... Step: 3370... Loss: 1.2105... Val Loss: 1.3553\n",
      "Epoch: 19/25... Step: 3380... Loss: 1.1203... Val Loss: 1.3486\n",
      "Epoch: 20/25... Step: 3390... Loss: 1.1490... Val Loss: 1.3414\n",
      "Epoch: 20/25... Step: 3400... Loss: 1.1209... Val Loss: 1.3481\n",
      "Epoch: 20/25... Step: 3410... Loss: 1.1913... Val Loss: 1.3512\n",
      "Epoch: 20/25... Step: 3420... Loss: 1.1544... Val Loss: 1.3488\n",
      "Epoch: 20/25... Step: 3430... Loss: 1.1888... Val Loss: 1.3482\n",
      "Epoch: 20/25... Step: 3440... Loss: 1.1517... Val Loss: 1.3599\n",
      "Epoch: 20/25... Step: 3450... Loss: 1.1631... Val Loss: 1.3599\n",
      "Epoch: 20/25... Step: 3460... Loss: 1.1690... Val Loss: 1.3543\n",
      "Epoch: 20/25... Step: 3470... Loss: 1.1532... Val Loss: 1.3461\n",
      "Epoch: 20/25... Step: 3480... Loss: 1.1436... Val Loss: 1.3466\n",
      "Epoch: 20/25... Step: 3490... Loss: 1.1697... Val Loss: 1.3478\n",
      "Epoch: 20/25... Step: 3500... Loss: 1.1482... Val Loss: 1.3557\n",
      "Epoch: 20/25... Step: 3510... Loss: 1.1474... Val Loss: 1.3482\n",
      "Epoch: 20/25... Step: 3520... Loss: 1.1428... Val Loss: 1.3481\n",
      "Epoch: 20/25... Step: 3530... Loss: 1.1639... Val Loss: 1.3460\n",
      "Epoch: 20/25... Step: 3540... Loss: 1.1649... Val Loss: 1.3459\n",
      "Epoch: 20/25... Step: 3550... Loss: 1.1901... Val Loss: 1.3481\n",
      "Epoch: 20/25... Step: 3560... Loss: 1.2163... Val Loss: 1.3443\n",
      "Epoch: 21/25... Step: 3570... Loss: 1.1494... Val Loss: 1.3456\n",
      "Epoch: 21/25... Step: 3580... Loss: 1.1294... Val Loss: 1.3521\n",
      "Epoch: 21/25... Step: 3590... Loss: 1.1487... Val Loss: 1.3504\n",
      "Epoch: 21/25... Step: 3600... Loss: 1.1680... Val Loss: 1.3476\n",
      "Epoch: 21/25... Step: 3610... Loss: 1.1572... Val Loss: 1.3442\n",
      "Epoch: 21/25... Step: 3620... Loss: 1.1631... Val Loss: 1.3503\n",
      "Epoch: 21/25... Step: 3630... Loss: 1.1591... Val Loss: 1.3511\n",
      "Epoch: 21/25... Step: 3640... Loss: 1.1784... Val Loss: 1.3463\n",
      "Epoch: 21/25... Step: 3650... Loss: 1.1476... Val Loss: 1.3409\n",
      "Epoch: 21/25... Step: 3660... Loss: 1.0930... Val Loss: 1.3463\n",
      "Epoch: 21/25... Step: 3670... Loss: 1.1634... Val Loss: 1.3422\n",
      "Epoch: 21/25... Step: 3680... Loss: 1.1427... Val Loss: 1.3480\n",
      "Epoch: 21/25... Step: 3690... Loss: 1.1568... Val Loss: 1.3488\n",
      "Epoch: 21/25... Step: 3700... Loss: 1.1249... Val Loss: 1.3429\n",
      "Epoch: 21/25... Step: 3710... Loss: 1.1616... Val Loss: 1.3469\n",
      "Epoch: 21/25... Step: 3720... Loss: 1.1545... Val Loss: 1.3424\n",
      "Epoch: 21/25... Step: 3730... Loss: 1.1673... Val Loss: 1.3463\n",
      "Epoch: 22/25... Step: 3740... Loss: 1.1304... Val Loss: 1.3424\n",
      "Epoch: 22/25... Step: 3750... Loss: 1.1208... Val Loss: 1.3457\n",
      "Epoch: 22/25... Step: 3760... Loss: 1.1219... Val Loss: 1.3476\n",
      "Epoch: 22/25... Step: 3770... Loss: 1.1480... Val Loss: 1.3489\n",
      "Epoch: 22/25... Step: 3780... Loss: 1.1349... Val Loss: 1.3479\n",
      "Epoch: 22/25... Step: 3790... Loss: 1.1666... Val Loss: 1.3453\n",
      "Epoch: 22/25... Step: 3800... Loss: 1.1400... Val Loss: 1.3433\n",
      "Epoch: 22/25... Step: 3810... Loss: 1.1163... Val Loss: 1.3467\n",
      "Epoch: 22/25... Step: 3820... Loss: 1.1485... Val Loss: 1.3482\n",
      "Epoch: 22/25... Step: 3830... Loss: 1.1485... Val Loss: 1.3361\n",
      "Epoch: 22/25... Step: 3840... Loss: 1.1569... Val Loss: 1.3393\n",
      "Epoch: 22/25... Step: 3850... Loss: 1.1440... Val Loss: 1.3380\n",
      "Epoch: 22/25... Step: 3860... Loss: 1.1189... Val Loss: 1.3428\n",
      "Epoch: 22/25... Step: 3870... Loss: 1.1571... Val Loss: 1.3461\n",
      "Epoch: 22/25... Step: 3880... Loss: 1.0997... Val Loss: 1.3456\n",
      "Epoch: 22/25... Step: 3890... Loss: 1.1158... Val Loss: 1.3436\n",
      "Epoch: 22/25... Step: 3900... Loss: 1.1685... Val Loss: 1.3406\n",
      "Epoch: 22/25... Step: 3910... Loss: 1.1420... Val Loss: 1.3458\n",
      "Epoch: 23/25... Step: 3920... Loss: 1.1250... Val Loss: 1.3424\n",
      "Epoch: 23/25... Step: 3930... Loss: 1.1377... Val Loss: 1.3422\n",
      "Epoch: 23/25... Step: 3940... Loss: 1.1461... Val Loss: 1.3416\n",
      "Epoch: 23/25... Step: 3950... Loss: 1.1461... Val Loss: 1.3444\n",
      "Epoch: 23/25... Step: 3960... Loss: 1.1637... Val Loss: 1.3441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/25... Step: 3970... Loss: 1.1109... Val Loss: 1.3426\n",
      "Epoch: 23/25... Step: 3980... Loss: 1.1000... Val Loss: 1.3393\n",
      "Epoch: 23/25... Step: 3990... Loss: 1.1773... Val Loss: 1.3373\n",
      "Epoch: 23/25... Step: 4000... Loss: 1.1495... Val Loss: 1.3447\n",
      "Epoch: 23/25... Step: 4010... Loss: 1.1545... Val Loss: 1.3402\n",
      "Epoch: 23/25... Step: 4020... Loss: 1.1130... Val Loss: 1.3417\n",
      "Epoch: 23/25... Step: 4030... Loss: 1.1317... Val Loss: 1.3400\n",
      "Epoch: 23/25... Step: 4040... Loss: 1.1373... Val Loss: 1.3419\n",
      "Epoch: 23/25... Step: 4050... Loss: 1.1283... Val Loss: 1.3434\n",
      "Epoch: 23/25... Step: 4060... Loss: 1.1407... Val Loss: 1.3374\n",
      "Epoch: 23/25... Step: 4070... Loss: 1.1412... Val Loss: 1.3511\n",
      "Epoch: 23/25... Step: 4080... Loss: 1.1311... Val Loss: 1.3405\n",
      "Epoch: 23/25... Step: 4090... Loss: 1.1337... Val Loss: 1.3411\n",
      "Epoch: 24/25... Step: 4100... Loss: 1.1571... Val Loss: 1.3445\n",
      "Epoch: 24/25... Step: 4110... Loss: 1.1055... Val Loss: 1.3329\n",
      "Epoch: 24/25... Step: 4120... Loss: 1.1130... Val Loss: 1.3448\n",
      "Epoch: 24/25... Step: 4130... Loss: 1.1481... Val Loss: 1.3439\n",
      "Epoch: 24/25... Step: 4140... Loss: 1.1343... Val Loss: 1.3414\n",
      "Epoch: 24/25... Step: 4150... Loss: 1.1148... Val Loss: 1.3428\n",
      "Epoch: 24/25... Step: 4160... Loss: 1.1472... Val Loss: 1.3399\n",
      "Epoch: 24/25... Step: 4170... Loss: 1.1579... Val Loss: 1.3367\n",
      "Epoch: 24/25... Step: 4180... Loss: 1.1296... Val Loss: 1.3388\n",
      "Epoch: 24/25... Step: 4190... Loss: 1.0966... Val Loss: 1.3430\n",
      "Epoch: 24/25... Step: 4200... Loss: 1.1508... Val Loss: 1.3421\n",
      "Epoch: 24/25... Step: 4210... Loss: 1.0970... Val Loss: 1.3432\n",
      "Epoch: 24/25... Step: 4220... Loss: 1.1341... Val Loss: 1.3348\n",
      "Epoch: 24/25... Step: 4230... Loss: 1.1230... Val Loss: 1.3314\n",
      "Epoch: 24/25... Step: 4240... Loss: 1.1290... Val Loss: 1.3363\n",
      "Epoch: 24/25... Step: 4250... Loss: 1.1341... Val Loss: 1.3414\n",
      "Epoch: 24/25... Step: 4260... Loss: 1.1601... Val Loss: 1.3361\n",
      "Epoch: 24/25... Step: 4270... Loss: 1.0808... Val Loss: 1.3407\n",
      "Epoch: 25/25... Step: 4280... Loss: 1.1110... Val Loss: 1.3393\n",
      "Epoch: 25/25... Step: 4290... Loss: 1.0725... Val Loss: 1.3415\n",
      "Epoch: 25/25... Step: 4300... Loss: 1.1443... Val Loss: 1.3418\n",
      "Epoch: 25/25... Step: 4310... Loss: 1.1159... Val Loss: 1.3414\n",
      "Epoch: 25/25... Step: 4320... Loss: 1.1416... Val Loss: 1.3393\n",
      "Epoch: 25/25... Step: 4330... Loss: 1.1083... Val Loss: 1.3418\n",
      "Epoch: 25/25... Step: 4340... Loss: 1.1264... Val Loss: 1.3431\n",
      "Epoch: 25/25... Step: 4350... Loss: 1.1245... Val Loss: 1.3385\n",
      "Epoch: 25/25... Step: 4360... Loss: 1.1210... Val Loss: 1.3342\n",
      "Epoch: 25/25... Step: 4370... Loss: 1.0939... Val Loss: 1.3431\n",
      "Epoch: 25/25... Step: 4380... Loss: 1.1281... Val Loss: 1.3474\n",
      "Epoch: 25/25... Step: 4390... Loss: 1.0930... Val Loss: 1.3492\n",
      "Epoch: 25/25... Step: 4400... Loss: 1.1163... Val Loss: 1.3392\n",
      "Epoch: 25/25... Step: 4410... Loss: 1.1093... Val Loss: 1.3335\n",
      "Epoch: 25/25... Step: 4420... Loss: 1.1230... Val Loss: 1.3359\n",
      "Epoch: 25/25... Step: 4430... Loss: 1.1241... Val Loss: 1.3355\n",
      "Epoch: 25/25... Step: 4440... Loss: 1.1297... Val Loss: 1.3383\n",
      "Epoch: 25/25... Step: 4450... Loss: 1.1594... Val Loss: 1.3422\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 100, 100\n",
    "\n",
    "train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    \n",
    "    h = net.init_hidden(1)\n",
    "    \n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        \n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patty, and something\n",
      "true, though simply he had the course, and he came into the figure\n",
      "what his face was impossible to be carried.\n",
      "\n",
      "And that he had been told his white the stopping seemed the cloth to go out\n",
      "of the most since and her first thing. He could not help having become that\n",
      "her husband had taken up to think of it, and was at a lady, and saw\n",
      "that he had a little arrived, he could not see her, she had bound, and\n",
      "had never seen and chestly it was that his highel peacor was at the first\n",
      "time that her hands and that in his hostility with the carriages and finests\n",
      "had tasted, she could not tell that show hardly of such the same thing that\n",
      "it was time to be calm, and a man--and will to be the same somathing about\n",
      "it, and he was not to blame from her state of the prince shook her forehead\n",
      "and sort of theater. The satisfaction had come.\n",
      "\n",
      "To him when she was to be said to a luduroth, and all that inseed one words\n",
      "and a child, the crowd, and the searer had said in society.\n",
      "\n",
      "The cried and true. But the part had never seen his bell, he saw if there had\n",
      "seem him inditable. He had not the same time he had begun still her sense,\n",
      "too, which had so mistaken a chance of come to step it to their husband\n",
      "with his from attimpte on the party of the same dismiseal day to\n",
      "dress. In the single side of the post of her husband's article whom he\n",
      "had been so minute, to be cheated further for his sore, he would have\n",
      "come off, but he could not help looking in his eyes to the words\n",
      "what had been talking to him, and he could not see how the most conscious\n",
      "that he could not be condified that her forming with that she had\n",
      "settled her at that time the painful position, how is the services of\n",
      "that carriages of the chair in any sort of heart seemed to Levin as though\n",
      "she could not come and discribe. But he was standing at once always and\n",
      "at little, who had been all at the same man had been to say. And would not have\n",
      "said at\n",
      "home, all the world it would be now that he could not carry the pase.\n",
      "\"I can'\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 2000, prime='Patty', top_k=5, cuda=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.3 64-bit",
   "language": "python",
   "name": "python36364bita492b5e99330473ea4e32e6291944bbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
